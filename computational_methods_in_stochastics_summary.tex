% --- LaTeX Lecture Notes Template - S. Venkatraman ---

% --- Set document class and font size ---

\documentclass[letterpaper, 12pt]{article}

% --- Package imports ---

\usepackage{tcolorbox}

\newcommand{\myexample}[2]{
    \begin{tcolorbox}[colback=black!5!white,colframe=black,title={Example: #1},
    before skip=12pt]
        #2
    \end{tcolorbox}
}

% Extended set of colors
\usepackage[dvipsnames]{xcolor}

\usepackage{
  amsmath, amsthm, amssymb, mathtools, dsfont, units,          % Math typesetting
  graphicx, wrapfig, subfig, float,                            % Figures and graphics formatting
  listings, color, inconsolata, pythonhighlight,               % Code formatting
  fancyhdr, sectsty, hyperref, enumerate, enumitem, framed }   % Headers/footers, section fonts, links, lists
  
  \usepackage{enumitem

% lipsum is just for generating placeholder text and can be removed
\usepackage{hyperref, lipsum} 

% --- Fonts ---

\usepackage{newpxtext, newpxmath, inconsolata}

% --- Page layout settings ---

% Set page margins
\usepackage[left=1.35in, right=1.35in, top=1.0in, bottom=.9in, headsep=.5in, footskip=0.35in]{geometry}

% Anchor footnotes to the bottom of the page
\usepackage[bottom]{footmisc}

% Set line spacing
\renewcommand{\baselinestretch}{1.2}

% Set spacing between paragraphs
\setlength{\parskip}{1.3mm}

% Allow multi-line equations to break onto the next page
\allowdisplaybreaks

% --- Page formatting settings ---

% Set image captions to be italicized
\usepackage[font={it,footnotesize}]{caption}

% Set link colors for labeled items (blue), citations (red), URLs (orange)
\hypersetup{colorlinks=true, linkcolor=RoyalBlue, citecolor=RedOrange, urlcolor=ForestGreen}

% Set font size for section titles (\large) and subtitles (\normalsize) 
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{{\fontsize{19}{19}\selectfont\textreferencemark}\;\; }{0em}{}
\titleformat{\subsection}{\normalsize\bfseries\selectfont}{\thesubsection\;\;\;}{0em}{}

% Enumerated/bulleted lists: make numbers/bullets flush left
%\setlist[enumerate]{wide=2pt, leftmargin=16pt, labelwidth=0pt}
\setlist[itemize]{wide=0pt, leftmargin=16pt, labelwidth=10pt, align=left}

% --- Table of contents settings ---

\usepackage[subfigure]{tocloft}

% Reduce spacing between sections in table of contents
\setlength{\cftbeforesecskip}{.9ex}

% Remove indentation for sections
\cftsetindents{section}{0em}{0em}
\setlength\parindent{0pt}

% Set font size (\large) for table of contents title
\renewcommand{\cfttoctitlefont}{\large\bfseries}

% Remove numbers/bullets from section titles in table of contents
\makeatletter
\renewcommand{\cftsecpresnum}{\begin{lrbox}{\@tempboxa}}
\renewcommand{\cftsecaftersnum}{\end{lrbox}}
\makeatother

% --- Set path for images ---

\graphicspath{{Images/}{../Images/}}

% --- Math/Statistics commands ---

% Add a reference number to a single line of a multi-line equation
% Usage: "\numberthis\label{labelNameHere}" in an align or gather environment
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Shortcut for bold text in math mode, e.g. $\b{X}$
\let\b\mathbf

% Shortcut for bold Greek letters, e.g. $\bg{\beta}$
\let\bg\boldsymbol

% Shortcut for calligraphic script, e.g. %\mc{M}$
\let\mc\mathcal

% \mathscr{(letter here)} is sometimes used to denote vector spaces
\usepackage[mathscr]{euscript}

% Convergence: right arrow with optional text on top
% E.g. $\converge[p]$ for converges in probability
\newcommand{\converge}[1][]{\xrightarrow{#1}}

% Weak convergence: harpoon symbol with optional text on top
% E.g. $\wconverge[n\to\infty]$
\newcommand{\wconverge}[1][]{\stackrel{#1}{\rightharpoonup}}

% Equality: equals sign with optional text on top
% E.g. $X \equals[d] Y$ for equality in distribution
\newcommand{\equals}[1][]{\stackrel{\smash{#1}}{=}}

% Normal distribution: arguments are the mean and variance
% E.g. $\normal{\mu}{\sigma}$
\newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}

% Uniform distribution: arguments are the left and right endpoints
% E.g. $\unif{0}{1}$
\newcommand{\unif}[2]{\text{Uniform}(#1,#2)}

% Independent and identically distributed random variables
% E.g. $ X_1,...,X_n \iid \normal{0}{1}$
\newcommand{\iid}{\stackrel{\smash{\text{iid}}}{\sim}}

% Sequences (this shortcut is mostly to reduce finger strain for small hands)
% E.g. to write $\{A_n\}_{n\geq 1}$, do $\bk{A_n}{n\geq 1}$
\newcommand{\bk}[2]{\{#1\}_{#2}}

% Math mode symbols for common sets and spaces. Example usage: $\R$
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\C}{\mathbb{C}} % Complex numbers
\newcommand{\Q}{\mathbb{Q}} % Rational numbers
\newcommand{\Z}{\mathbb{Z}} % Integers
\newcommand{\N}{\mathbb{N}} % Natural numbers
\newcommand{\F}{\mathcal{F}}  % Calligraphic F for a sigma algebra
\newcommand{\El}{\mathcal{L}} % Calligraphic L, e.g. for L^p spaces

% Math mode symbols for probability
\newcommand{\pr}{\mathbb{P}}  % Probability measure
\newcommand{\E}{\mathbb{E}} % Expectation, e.g. $\E(X)$
\newcommand{\var}{\text{Var}} % Variance, e.g. $\var(X)$
\newcommand{\cov}{\text{Cov}} % Covariance, e.g. $\cov(X,Y)$
\newcommand{\corr}{\text{Corr}} % Correlation, e.g. $\corr(X,Y)$
\newcommand{\B}{\mathcal{B}}  % Borel sigma-algebra

% Other miscellaneous symbols
\newcommand{\tth}{\text{th}}  % Non-italicized 'th', e.g. $n^\tth$
\newcommand{\Oh}{\mathcal{O}} % Big-O notation, e.g. $\O(n)$
\newcommand{\1}{\mathds{1}} % Indicator function, e.g. $\1_A$

% Additional commands for math mode
\DeclareMathOperator*{\argmax}{argmax}    % Argmax, e.g. $\argmax_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\argmin}{argmin}    % Argmin, e.g. $\argmin_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\spann}{Span}   % Span, e.g. $\spann\{X_1,...,X_n\}$
\DeclareMathOperator*{\bias}{Bias}    % Bias, e.g. $\bias(\hat\theta)$
\DeclareMathOperator*{\ran}{ran}      % Range of an operator, e.g. $\ran(T) 
\DeclareMathOperator*{\dv}{d\!}     % Non-italicized 'with respect to', e.g. $\int f(x) \dv x$
\DeclareMathOperator*{\diag}{diag}    % Diagonal of a matrix, e.g. $\diag(M)$
\DeclareMathOperator*{\trace}{trace}    % Trace of a matrix, e.g. $\trace(M)$
\DeclareMathOperator*{\supp}{supp}    % Support of a function, e.g., $\supp(f)$

% Numbered theorem, lemma, etc. settings - e.g., a definition, lemma, and theorem appearing in that 
% order in Lecture 2 will be numbered Definition 2.1, Lemma 2.2, Theorem 2.3. 
% Example usage: \begin{theorem}[Name of theorem] Theorem statement \end{theorem}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

% Un-numbered theorem, lemma, etc. settings
% Example usage: \begin{lemma*}[Name of lemma] Lemma statement \end{lemma*}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}
\newtheorem*{remark*}{Remark}
\newtheorem*{claim}{Claim}

% --- Left/right header text (to appear on every page) ---

% Do not include a line under header or above footer
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}
% Right header text: Lecture number and title
\renewcommand{\sectionmark}[1]{\markright{#1} }
\fancyhead[R]{\small\textit{\nouppercase{\rightmark}}}

% Left header text: Short course title, hyperlinked to table of contents
\fancyhead[L]{\hyperref[sec:contents]{\small Computational Methods in Stochastics}}

% --- Document starts here ---

\begin{document}

% --- Main title and subtitle ---

\title{Computational Methods in Stochastics \\[1em]
  \normalsize CS-E5795: Summarised lecture notes \color{red}{\textbf {DRAFT}}}

% --- Author and date of last update ---

\author{\normalsize All content based on the lecture notes by Prof. Riku Linna (Aalto University).\\[-0.4em]
  \normalsize Summarised and edited by Jieming You. \\ \dots}
\date{\normalsize\vspace{-1ex} Last updated: \today}

% --- Add title and table of 

\maketitle
% --- Main content: import lectures as subfiles ---
\section{Random variables and distributions}

\subsection{Random variables}

The expression ${X \leq x}$ is the event that random variable $X$ assumes a value lesser or equal to the real number $x$. The probability of this event is $\Pr(X \leq x)$.

The cumulative probability distribution of random variable $X$ is defined as
\[
  F(x) = \Pr (X \leq x), \quad -\infty < x < \infty
\]
Similarly the probability of the event of $X > x$ is $\Pr (X > x) = 1-F(x)$.

For continuous random variable $X$: $\Pr(X = x) = 0 \quad \forall x$.

Probability density function of random variable $X$ is defined as a nonnegative function
\[
  \Pr(a < X \leq b) = \int_a^b f(x)~dx \qquad \text{for} -\infty < a < b < \infty
\]

Consequently, PDF is the derivative of the CDF.

\subsubsection{Expectation of random variables}

If $X$ is a random bariable, $Y = g(X)$  is also a random varialble. The expectation ("expected value") of $Y$ is
\begin{align*}
  \E[g(x)] & = \int g(x)f_X(x)~dx
\end{align*}

\subsubsection{Joint distributions, independency, and conditionality}

Given two random variables $X$ and $Y$, their joint distribution is defined as
\begin{align*}
  F_{XY}(x, y)
    & = F(x, y)                                                                           \\
    & = \Pr(X \leq x ~\text{and}~ Y \leq y)                                               \\
    & = \int_{-\infty}^x \int_{-\infty}^yf_{XY}(\xi, \eta)~ d\xi d\eta \quad \forall x, y
\end{align*}

For jointly distributed random variables, $\E[X + Y] = \E[X] + \E[Y]$.

Random variables $X$ and $Y$ are called independent if $F(x,y) = F_X(x)F_Y(y)$.

For all events $A$ and $B$ where $\Pr(B) > 0$, the conditional probability of $A$ given $B$ is defined by the Bayes Theorem:
\[
  \Pr(A | B) = \frac{\Pr(A \cap B)}{\Pr(B)}, \quad \Pr(B) > 0
\]

This relates to the law of total probability
\[
  \Pr(A) = \sum_i^\infty\Pr(A \cap B_i) =  \sum_i^\infty\Pr(A | B_i) \Pr(B_i)
\]
% --- Document ends here ---

\subsection{Central limit theorem}

An intuitive definition of the central limit theorem: The independent observations of any random process with fixed mean and variance (homogenous) are normally distributed with the original mean. This is called an \textbf{additive process}.

Similarly, the lognormal distribution is the result of a \textbf{multiplicatively processes}. That is, the random process is described by a product (instead of a sum in the case of the normal distribution) of i.i.d. observations.

\section{Stochastic simulation}

\subsection{Monte Carlo}

The expected value of a random variable can be calculated by integrating over the probability density functions $\E[g(x)] & = \int g(x)f_X(x)~dx$. In the case of more complicated distributions and functions, it's seldom trivial to calculate the integral analytically.

Monte Carlo integration is a form of stochastic simulation, where we approximate the integral of $g(X)$ by taking random samples from $g(x)$ as $g(x_i), ..., g(x_n)$. The law of large numbers assure that the integral can be numerically approximated by
\begin{align*}
  E(g(X))
    & = \int \color{red}{g(x)} \color{blue}{f_X(x)~dx}                                     \\
    & \approx \frac1n \sum_{i=1}^n \color{red}{g(x_i)} \quad \color{blue}x_i \sim f(\cdot)
\end{align*}

This can be applied to problems where $X$ is also difficult to sample but we can sample the realisations of $Y$ that has a PDF $f_Y(\cdot)$
\begin{align*}
  E(g(X))
    & = \int {g(x)} {f_X(x)~dx}                                                                   \\
    & = \int \frac{{g(x)} {f_X(x)}}{f_Y(x)} f_Y(x)~dx                                             \\
    & \approx \frac1n \sum_{i=1}^n \frac{{g(y_i)} {f_X(y_i)}}{f_Y(y_i)} \quad y_i \sim f_Y(\cdot)
\end{align*}

\subsection{Transformation methods}

\subsubsection{Inverse distribution}

When an inverse of a CDF can be calculated, the probability density function of a random variable can be sampled from the unversed CDF with using a uniform distribution $y \sim U(0, 1)$.

\myexample{Sampling exponential random variates using y \sim U(0, 1)}
{
$F(x) = 1 - e^{-\lambda x} \Rightarrow F^{-1}(y) = -\frac1\lambda \ln(1-y)$.
Samples $x \sim Exp(\lambda)$ can be calculated as simply as
\[
  x = F^{-1}(y) \quad y \sim U(0, 1)
\]
}
\subsubsection{Rejection sampling}

If we want to simulate a distribution $f(x)$ with support on $[a, b]$ and an determined upper bound $m$ such that $f(x) \leq m, \forall x \in [a, b]$, we can simulate the distribution by drawing the value for x-axis from $x \sim U(a,b)$ and then drawing the value for y-axis from $y \sim U(0, m)$, and accept the drawn $x$ if $y < f(x)$. The accepted $X$ values will have a PDF f(x).

Intuition: We assume that the acceptance probability is proportional to the probability density when using rejection sampling. \\

Envelope method is an extension of the rejection sampling method that is applicable to distributions with infinite support. We use another distribution $ah(x)$ to "cover/envelope" the distribution $f(x)$ to be sampled. First, we will draw a point along the y-axis $y \sim h(\cdot)$ and another point along the y-axis $u\sim U(0, ah(y))$ with a greater range. We will accept $y$ as the simulated value of the target distribution if $u < f(y)$.


\section{Conditionality and Markov Processes}

The conditional expected value in the discrete case was defined as
\[
  \Pr(X=x) = \sum_{y=0}^\infty P_{X|Y}(x|y)p_Y(y)
\]

Then, the conditional expected value of $g(X)$ given $Y=y$ is
\[
  \E[g(X) | Y=y] = \sum_x g(x) p_{X|Y}(x|y) \quad \text{if } p_Y(y) > 0
\]

Likelihood $L(\theta | \bf{x})$ gives the likelihood of parameter $\theta$ given the observed data $\bf{x}$.
\[
  L(\theta | \bf{x}) = \prod_{i=1}^n f(x_i | \theta)
\]

The maximum likelihood estimator (MLE) $\hat{\theta}$ is the most likely $\theta$ given the available data $\bf{x}$.

\subsection{Markov Chains properties}

The one-step transition probability in is defined as
\[
  P_{ij}^{n, n+1} = \Pr\{X_{n+1} = j | X_n = i\}
\]

Transition matrix $ \bf P \in \R^{i \times j}$ stores the transition probabilities from state $i$ to state $j$. The transition probabilities $i \rightarrow j$ form a probability mass distribution and the transition probabilities together holds the property of being equal to 1 when summed.

The $n$-step transition probability matrices is
\[
  \bf P^{(n)} = \left[ P_{ij}^{(n)} \right]
\]

The probability of initially being in state $j$ is $\Pr(X_0 = j)=p_j$. Thus, the probability of being at state $k$ at time $n$ can be expressed as
\[
  \Pr(X_n = j | X_0 = i) = \sum_{j} \bf{P}_j \bf{P}_{jk}^{n} \quad
\]

A distribution is said to be a stationary distribution of the homogeneous Markov chain if
\[\pi = \pi\bf{P}\]

$\pi$ is a row vector denoting the probabilities of being at each state $\pi^{(t)} = \left( \pi^{(t)}(x_1), ..., \pi^{(t)}(x_j) \right)$. If a Markov chains reaches a stationary distribution, it retains for all future time.

The stationary distribution can be solved from
\[
  \pi(I-P) = 0
\]

\subsection{Markov Chains in continuous time}

In continuous time, a transition kernel can be written $p(x, x^\prime, t^\prime)$. We can write a transition matrix for each $t^\prime$.

The transition matrix $Q$ can be seen as a "flow" probability matrix which is the time derivative of the transition matrix P(t^\prime).
\[
  Q = \frac{d}{dt^\prime} P(t^\prime)\vert_{t^\prime=0}
\]

When solving for stationary distribution $\pi$, use $\pi Q = 0$ and \sum_i \pi_i = 1.

\myexample{Discrete event simulation}{
\begin{enumerate}[itemsep=-1pt]
  \item Initialize the process at $t=0$ with initial $\text{state} = i$
  \item Obtain the flow parameter $q_{ii}$
  \item Simulate the time to the next event $t^\prime$ as $Exp(-q_{ii})$
  \item Assign $t:= t + t^\prime$ and $\text{state} = j$
\end{enumerate}
}

\vspace{5mm}

\myexample{Immigration-death process}
{
Population grows by 1 with constant hazard $\lambda$. Each individual dies independently with constant hazard $\mu$. The possible transition are
\begin{enumerate}[itemsep=-1pt]
  \item $P\{~X(t + dt) = x+1 | X(t)=x~\} = \lambda dt$
  \item $P\{~X(t + dt) = x-1 | X(t)=x~\} = x\mu dt$
  \item $P\{~X(t + dt) = x | X(t)=x~\} = 1 - (\lambda + x\mu) dt$
\end{enumerate}
These equations define a homogeneous Markov process with infinite state-space $S$. The stationary distribution of this process is a Poisson distribution with mean $\lambda/\mu$.
}

In an inhomogeneous Poisson process, the hazard $\lambda$ is changing w.r.t. time $\lambda(t)$. Cumulative hazard is defined as the total hazard within a timespan
\[
  \Lambda (t) = \int_0^t \lambda(s)~ds
\]

The number of events in the interval $(s, t]$: $0 < s < t$ is $Po(\Lambda(t) - \Lambda(s))$.

\myexample{Inhomogeneous Poisson process}
{

Consider an inhomogeneous Poisson process with rate parameter $\lambda(t) = \lambda t $ for some $t > 0$. The cumulative hazard of the process is
\[
  \Lambda (t) = \int \lambda t~dt = \frac12 \lambda t^2
\]

Thus, the number of events $N_t \sim Po(\Lambda(t)) = Po(\frac12 \lambda t^2)$. The number of events in the interval $(s,t]$ is then $Po(\frac12 \lambda (t^2 - s^2))$.
}

\subsection{Sampling an inhomogeneous Poisson process}

The lectures present two algorithms for sampling an inhomogeneous Poisson process on $(0, T]$ with rate parameter $\lambda(t)$ given that we find an upper bound $U_\lambda$ satisfying $0 \leq \lambda(t) \leq U_\lambda$.

\subsubsection{The "revised" algorithm}

In the revised algorithm, we sample the inhomogeneous Poisson process by simulating the number of observation happening at each timestep. This ensures that all sampled events are in order.

\myexample{The "revised" algorithm}{
\begin{enumerate}[itemsep=-1pt]
  \item Set $x_0 := 0$
  \item Sample $m \sim Po(U_\lambda T)$
  \item For $i := 1, ..., m$
        \subitem (a) sample $u \sim U(0, 1)$
        \subitem (b) Set $x_i := x_{i-1} + (T - x_{i-1})(1-u^{1/(m-i+1)})$
        \subitem (c) Sample $y \sim U(0, U_\lambda)$
        \subitem (d) Accept $x_i$ if $y \leq \lambda(x_i)$
\end{enumerate}
}

\subsubsection{Thinning method}

In the thinning method, we use the exponential distribution to sample the inter-event times for the x-axis. The point for the y-coordinate is sampled from a $U(0, U_\lambda)$ and accepted if within $\Omega$.

\myexample{Thinning method}{
\begin{enumerate}[itemsep=-1pt]
  \item Set $t_0 := 0$
  \item Set $i := 1$
  \item Repeat
        \subitem (a) Sample $t \sim Exp(U_\lambda)$
        \subitem (b) Set $t := t_{i-1} + t$
        \subitem (c) Stop if $t_i > T$
        \subitem (d) Sample from $u \sim U(0,U_\lambda)$
        \subitem (e) Keep $t_i$ if u $\leq \lambda(t_i)$
        \subitem (f) Set $i := i+1$
\end{enumerate}
}

\section{MCMC and Bayesian Inference}

\begin{definition}
  A Markov chain Monte Carlo (MCMC) method for the simulation of a distribution $f$ is any method producing an ergodic Markov chain $X^{(t)}$ whose stationary distribution is $f$.
\end{definition}

When we measure an outcome $X=x$, we are interested in the conditional probability of the hypothesis given the realisation of $X$, which is $P(H_i | X=x)$.

We use the occurrence $X=x$ to calculate the posterior belief using our prior knowledge using the Bayes Theorem.
\[
  P(H_i | X=x)
  = \frac{P(X=x | H_i) P(H_i)}{\sum_j P(X=x | H_j) P(H_j)}
\]
To generalise the idea for also continuous cases, the posterior probability $\pi(\theta | X=x)$ can be expressed as
\[
  \pi(\theta | X=x) = \frac{\pi(\theta)L(\theta;x)}{\int_\theta P(X=x | \theta^\prime)\pi(\theta^\prime) ~ d\theta^\prime}
\]

Because the denominator is just a constant of proportionality, we can express the posterior simply being proportional to the product of prior and the likelihood
\[
  \text{Posterior} \sim \text{Prior} \times \text{Likelihood}
\]

It is, however, not a trivial calculation for most of the non-trivial problems. We need a way to compute posterior densities without any analytical integrations (Hence the MCMC methods...)

\subsection{Gibbs Sampler}
Gibbs sampler is an MCMC method for simulating multivariate posterior distributions using the parameters' full conditional distributions.

For a bivariate density $\pi(x, y)$, we can use the law of total probability to decompose it into conditional probabilities w.r.t. $x$ and $y$
\begin{align*}
  \mathbf{1)} \quad \pi(x, y) & = \pi(x) \pi(y | x) \\
  \mathbf{2)} \quad \pi(x, y) & = \pi(y) \pi(x | y)
\end{align*}
In order to sample from $\pi(x, y)$, we would
\begin{enumerate}[itemsep=-1pt]
  \item First draw $x \sim \pi(x_o)$ and then $y \sim \pi(y_0 | x)$
  \item Then we draw $y \sim \pi(y_0)$ and obtain $x^\prime \sim \pi(x | y)$
  \item Finally, obtain $y^\prime \sim \pi(y | x^\prime)$
\end{enumerate}

Now we have drawn a point $(x^\prime, y^\prime) \sim \pi(x, y)$ using the full conditionals using the method used by the Gibbs sampler.

\subsection{Metropolis-Hastings algorithm}
Usually, the closed forms of the full conditional distributions are hard to or impossible to compute. Another MCMC algorithm is called Metropolis-Hastings.

In Metropolis-Hastings, we simulate the target distribution $\pi(\theta)$ using a proposal distribution $q(\cdot, \theta)$. The sampled value will be either accepted or rejected, based on an acceptance probability.

\myexample{The Metropolis-Hastings algorithm}{
\begin{enumerate}[itemsep=-1pt]
  \item Initiate the counter $j$ and parameter $\theta$
  \item Generate a proposal value $\phi \sim q(\theta^{j-1}, \phi)$
  \item Evaluate the acceptance probability $\alpha = \min\left\{ 1, \frac{\pi(\theta) q(\phi, \theta)}{\pi(\phi) q(\theta, \phi)} \right\}$
  \item Step $\theta ^{(j)} = \phi$ with probability $\alpha$, otherwise $\theta ^{(j)} = \theta ^{(j-1)}$
\end{enumerate}
}

When calculating the acceptance probability $\alpha$, it's worth noting that if the proposal distribution is symmetric, the probabilities $q(\phi, \theta) = q(\theta, \phi)$ and they will cancel out.

Any distribution $q(\cdot)$ can be used as the proposal distribution, but the choice of the proposal distribution will greatly affect the acceptance rate and distance covered each MH-step. A usual choice is a normal distribution
\[
  q(\cdot) \sim \mathrm{N}(\mu, 1)\] where the mean parameter is chose as $\mu = \theta$. When using a normal distribution, MH-algorithm samples the target distribution like a random walker with the distance covered equaling to $\sqrt{k}$ where k is the number of steps.

\section{Hamiltonian Monte Carlo}

The Hamiltonian Monte Carlo method uses Hamiltonian dynamics (from mechanics) combined with Metropolis sampling to construct an MCMC method.

\subsection{Hamiltonian dynamics}
The Hamiltonian function
\[
  H(q, p) = \underbracket{U(q)}_{\substack{\text{potential} \\ \text{energy}}} + \underbracket{K(p)}_{\substack{\text{kinetic} \\ \text{energy}}} = E_{tot}
\]
Gives the total and constant energy of the system, which consists of the potential energy and the kinetic energy.

In order to understand how the model's behaviour change over time $t$, we define the Hamilton's equations
\[
  \frac{d q_i}{dt} = \frac{\partial H}{\partial p_i}, \qquad
  \frac{d p_i}{dt} = -\frac{\partial H}{\partial q_i}
\]


The kinetic energy's definition is borrowed from the mechanics and can be modelled as
\[
  K(p) = \frac{p^T M^{-1} p}{2}
\]
where $M$ is a positive-definite matrix representing the weight of the "particle".

Hamiltonian stays invariant as the energy of the system stays constant.

\subsection{Discretisation of Hamilton's equations}

Hamilton's equations can be solved numerically using different discretisation methods, leapfrog integration being the most used one.

\myexample{The leapfrog method}{
Propagate $p_i$ in half-steps.
\begin{enumerate}
  \item $p_i(t + \epsilon/2) = p_i(t) - (\epsilon/2)\frac{\partial U}{\partial q_i}(q(t))$
  \item $q_i(t+\epsilon) = q_i(t) + \epsilon \frac{p_i(t+\epsilon/2)}{m_i}$
  \item $p_i(t + \epsilon) = p_i(t + \epsilon/2) - (\epsilon/2)\frac{\partial U}{\partial q_i}(q(t + \epsilon))$
\end{enumerate}
}

In short, the target of HMC is to \textbf {translate the density function of the target distribution to the potential energy function} and introduce momentum variables to go with the original variable(s) of interest. Finally, using a Markov Chain for each iteration, resample the momentum and \textbf {perform a Metropolis update with a proposal found using Hamiltonian dynamics}.

Imagine a probability distribution with a form of a skate pool. The depth of the pool equals to the probability density of the distribution. Using HMC, we will simulate the shape of the skate pool by placing a hockey puck on the ground, and kick it around the skate pool. The momentum of the puck is modelled by the kinetic energy and the position of the puck by the parameters of the potential energy function. The points where the puck lands after each kick is equals the proposal in the HMC sampling.

Coming back to the formal definition, the density function can be translated to potential energy function accordingly:
\begin{align*}
  H(q, p) & = -\log[\pi(p | q)] - \log[\pi(q)] \\
          & \equiv K(q, p) + V(q)
\end{align*}

\myexample{A single iteration of HMC}{
\begin{enumerate}
  \item Initiate $q^\ast = q_0$
  \item Sample $p_0 \sim N(0, 1);~ p^\ast := p_0$
  \item Leapfrog integration
        \subitem  Half step for momentum \\
        $~\qquad p^\ast := p^\ast - (\epsilon / 2) \cdot dU(q^\ast)/dq$
        \subitem Full step for position \\
        $~\qquad q^\ast := q^\ast - \epsilon \cdot p^\ast$
        \subitem  Half step for momentum \\
        $~\qquad p^\ast := p^\ast - (\epsilon / 2) \cdot dU(q^\ast)/dq$
  \item Negate the momentum at the end $p^\ast := -p^\ast$
  \item Evaluate the change in potential and kinetic energy
        \subitem $U_0 = U(q_0) \qquad K_0 = \frac12 p_0^2$
        \subitem $U^\ast = U(q^\ast) \qquad K^\ast = \frac12 p^\ast^2$
  \item Accept or reject proposal
        \subitem $u \sim U(0, 1)$
        \subitem $\text{if } u < \exp(U_0-U^\ast+K_0-K^\ast) \text{ then }q^\ast=q^\ast$
        \subitem else $ q^\ast = q^0$
\end{enumerate}
}

If the target distribution doesn't have the full support $(-\infty, \infty)$, we can introduce constraints (boundaries) by treating them as fully elastic collisions, i.e. the energies are mirrored with respect to the boundaries.

\end{example}





\end{document}